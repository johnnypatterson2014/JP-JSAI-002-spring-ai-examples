package com.calamansi.demo.integration;

import java.util.List;

import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.ai.chat.client.ChatClient;
import org.springframework.ai.chat.client.ChatClient.CallResponseSpec;
import org.springframework.ai.chat.client.advisor.MessageChatMemoryAdvisor;
import org.springframework.ai.chat.memory.InMemoryChatMemory;
import org.springframework.ai.chat.model.ChatResponse;
import org.springframework.stereotype.Component;

import com.calamansi.demo.config.ApplicationConfig;
import com.calamansi.demo.controller.ChatController;
import com.calamansi.demo.model.Answer;
import com.calamansi.demo.model.Itinerary;

import lombok.extern.slf4j.Slf4j;
import reactor.core.publisher.Flux;

import static org.springframework.ai.chat.client.advisor
.AbstractChatMemoryAdvisor.CHAT_MEMORY_CONVERSATION_ID_KEY;

@Component
@Slf4j
public class OpenAiIntegration {
	
	private static final Logger log = LoggerFactory.getLogger(OpenAiIntegration.class);

	private final ChatClient chatClient;

	public OpenAiIntegration(ChatClient.Builder builder) {
		// chat without memory
		//this.chatClient = builder.build();
		
		// chat with memory
		this.chatClient = builder
	        .defaultAdvisors(new MessageChatMemoryAdvisor(new InMemoryChatMemory()))
	        .build();
		
		// specify a system prompt
		// this.chatClient = builder.defaultSystem("You are a friendly chat bot that answers question in the voice of a Pirate").build();
		//
		// system prompt with template variables
		// builder.defaultSystem("You are a friendly chat bot that answers question in the voice of a {voice}").build();
		// and when calling the chatclient:
		// this.chatClient.prompt()
		//		.system(sp -> sp.param("voice", voice))
		//		.user(message)
		//		.call()
		//		.content());
	}

	public String callLLM(String message) {
		CallResponseSpec callResponseSpec = chatClient.prompt().user(message).call();
		ChatResponse response = callResponseSpec.chatResponse();
		String responseString = response.getResult().getOutput().getText();
		return responseString;
	}

	public ChatResponse callLLMWithChatResponse(String message) {
		
		CallResponseSpec callResponseSpec = chatClient.prompt().user(message).call();
		
		// there are different options for the return object when calling the LLM:
		// String textResponse = callResponseSpec.content();
		// ChatResponse chatResponse = callResponseSpec.chatResponse();
		// callResponseSpec.entity() to return a Java type:
		//		.entity(ParameterizedTypeReference<T> type): used to return a Collection of entity types
		//		.entity(Class<T> type): used to return a specific entity type
		// 		.entity(StructuredOutputConverter<T> structuredOutputConverter): used to specify an instance of a StructuredOutputConverter to convert a String to an entity type
		
		// You can also invoke the stream() method instead of call():
		//		Flux<String> content(): Returns a Flux of the string being generated by the AI model
		//		Flux<ChatResponse> chatResponse(): Returns a Flux of the ChatResponse object, which contains additional metadata about the response
		
		return callResponseSpec.chatResponse();
	}

	/* -----------------------------------------------------------
	 * You can easily map the AI modelâ€™s output to this record using the entity() method, as shown below:
	  
	 record ActorFilms(String actor, List<String> movies) {}
	  
	 ActorFilms actorFilms = chatClient.prompt()
	    .user("Generate the filmography for a random actor.")
	    .call()
	    .entity(ActorFilms.class);
	    
	 // -----------------------------------------------------------
	 // example to return a list of custom objects
	 
	 List<ActorFilms> actorFilms = chatClient.prompt()
	    .user("Generate the filmography of 5 movies for Tom Hanks and Bill Murray.")
	    .call()
	    .entity(new ParameterizedTypeReference<List<ActorFilms>>() {});
    
     // -----------------------------------------------------------
	 // example to stream the response
	 
     Flux<String> output = chatClient.prompt()
	    .user("Tell me a joke")
	    .stream()
	    .content();
	    
	 // -----------------------------------------------------------
	 // You can also stream the ChatResponse using the method Flux<ChatResponse> chatResponse()
    
     	var converter = new BeanOutputConverter<>(new ParameterizedTypeReference<List<ActorsFilms>>() {});

		Flux<String> flux = this.chatClient.prompt()
		    .user(u -> u.text("""
		                        Generate the filmography for a random actor.
		                        {format}
		                      """)
		            .param("format", this.converter.getFormat()))
		    .stream()
		    .content();
		
		String content = this.flux.collectList().block().stream().collect(Collectors.joining());
		
		List<ActorFilms> actorFilms = this.converter.convert(this.content);
    
    
    // -----------------------------------------------------------
    // Prompt Templates
    
    
	String answer = ChatClient.create(chatModel).prompt()
	    .user(u -> u
	            .text("Tell me the names of 5 movies whose soundtrack was composed by {composer}")
	            .param("composer", "John Williams"))
	    .call()
	    .content();
    
    
    
	 * 
	 */
	
	
	
	/*
	 * -----------------------------------------------------------
	 * How to call the LLM manually
	 * -----------------------------------------------------------
	 
	 
    public static void main(String[] args) throws IOException, InterruptedException {
        var apiKey = System.getenv("OPENAI_API_KEY");
        var body = """
                {
                    "model": "gpt-4o",
                    "messages": [
                        {
                            "role": "user",
                            "content": "Tell me an interesting fact about the Spring Framework"
                        }
                    ]
                }""";

        var request = HttpRequest.newBuilder()
                .uri(URI.create("https://api.openai.com/v1/chat/completions"))
                .header("Content-Type", "application/json")
                .header("Authorization", "Bearer " + apiKey)
                .POST(HttpRequest.BodyPublishers.ofString(body))
                .build();

        var client = HttpClient.newHttpClient();
        var response = client.send(request, HttpResponse.BodyHandlers.ofString());
        System.out.println(response.body());
    }
	 
	 * 
	 */
	
	
	public Flux<String> queryStream(String message) {
		return chatClient.prompt()
	            .user(message)
	            .stream()
	            .content();
	}

	public Answer chat(String message, String conversationId) {
		return chatClient.prompt()
        .user(message)
        .advisors(spec -> spec.param(CHAT_MEMORY_CONVERSATION_ID_KEY, conversationId))
        .call()
        .entity(Answer.class);
		//return chatClient.prompt().user(message).call().chatResponse();
	}
	
	public Itinerary vacationStructured() {
	    return chatClient.prompt()
	            .user("What's a good vacation plan while I'm in Montreal CA for 4 days?")
	            .call()
	            .entity(Itinerary.class);
	}
	
	public ChatResponse chatWithMemory(String message) {
		return chatClient.prompt().user(message).call().chatResponse();
	}

}
